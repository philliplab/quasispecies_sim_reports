---
title: Subsampling dmat
output: 
  bookdown::gitbook:
    config:
      toc:
        collapse: subsection
      sharing:
        facebook: no
        twitter: no
      fontsettings:
        theme: sepia
---

```{r, include=FALSE}
library(yasss)
library(knitr)
library(rmarkdown)
library(ggthemr)

knitr::opts_chunk$set(echo = FALSE,
                      base.dir = "/home/phillipl/projects/quasispecies_sim_reports/builds/subsampling_dmat/figures")
options(scipen = 99)
setCacheDir("/home/phillipl/projects/quasispecies_sim_reports/builds/subsampling_dmat/simpleCache")
set.seed(1)
source('../utilities/reporting_utilities.R')
```

```{r debugging-chunk, eval = FALSE}
yasss:::restart_r()
```

# Subsampling dmat

## Description

Storing large distance matrices consumes too much memory.

Can you subsample say 1 million observations from a distance matrix and discard everything else and still get a highly accurate density plot, decile estimates and average pairwise distance estimates?

```{r, include=FALSE}
args1 <- list(
  ancestors = paste(rep("A", 500), collapse = ''),
  r0 = 2,
  n_gen = 12,
  n_pop = Inf,
  mutator = list(fun = "mutator_uniform_fun",
                 args = list(mu = 1/250)),
  fitness_evaluator = list(fun = "fitness_evaluator_homology_fun",
                           args = list(comparators = paste(rep('XXXXA', 100), collapse = ''),
                                       h2fs = "h2fs_univariate_linear_fun"))
)

x <- memoiseCache(fun = 'sim_pop', args = args1, cacheName = 'subsampling_dmat', 
                  seed = 1)
```

```{r, include = FALSE}
last_gen <- x %>% filter(gen_num == max(gen_num))
dmat <- stringdistmatrix(last_gen$the_seq, method = 'hamming')

dmat_comp_tab <- data.frame(size = numeric(0),
                            avg_hd = numeric(0),
                            p000 = numeric(0),
                            p010 = numeric(0),
                            p020 = numeric(0),
                            p030 = numeric(0),
                            p040 = numeric(0),
                            p050 = numeric(0),
                            p060 = numeric(0),
                            p070 = numeric(0),
                            p080 = numeric(0),
                            p090 = numeric(0),
                            p100 = numeric(0)
                            )

for (i in 4:8){
  print(i)
  if (10^i < length(dmat)){
    subsam <- sample(as.numeric(dmat), 10^i)
    avg_hd <- mean(subsam)
    deciles <- t(data.frame(quantile(subsam, (0:10)/10)))
    names(deciles) <- c("p000", "p010", "p020", "p030", "p040", "p050", "p060",
                        "p070", "p080", "p090", "p100")
    row.names(deciles) <- NULL
    dmat_comp_tab <- rbind(dmat_comp_tab,
                           cbind(data.frame(size = i, 
                                            avg_hd = avg_hd),
                                 deciles))

  } else {
    avg_hd <- mean(dmat)
    deciles <- t(data.frame(quantile(dmat, (0:10)/10)))
    names(deciles) <- c("p000", "p010", "p020", "p030", "p040", "p050", "p060",
                        "p070", "p080", "p090", "p100")
    row.names(deciles) <- NULL
    dmat_comp_tab <- rbind(dmat_comp_tab,
                           cbind(data.frame(size = length(dmat), 
                                            avg_hd = avg_hd),
                                 deciles))
    break
  }
}
print(dmat_comp_tab)
```

What about subsampling on the sequences to speed up the computation of the distance matrix?

```{r}
```

