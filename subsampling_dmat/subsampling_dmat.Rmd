---
title: Subsampling dmat
output: 
  bookdown::gitbook:
    config:
      toc:
        collapse: subsection
      sharing:
        facebook: no
        twitter: no
      fontsettings:
        theme: sepia
---

```{r, include=FALSE}
library(yasss)
library(knitr)
library(rmarkdown)
library(ggthemr)

knitr::opts_chunk$set(echo = FALSE,
                      base.dir = "/home/phillipl/projects/quasispecies_sim_reports/builds/subsampling_dmat/figures")
options(scipen = 99)
setCacheDir("/home/phillipl/projects/quasispecies_sim_reports/builds/subsampling_dmat/simpleCache")
set.seed(1)
source('../utilities/reporting_utilities.R')
```

```{r debugging-chunk, eval = FALSE}
yasss:::restart_r()
```

# Subsampling dmat

## Description

Storing large distance matrices consumes too much memory.

Can you subsample say 1 million observations from a distance matrix and discard everything else and still get a highly accurate density plot, decile estimates and average pairwise distance estimates?

```{r, include=FALSE}
args1 <- list(
  ancestors = paste(rep("A", 500), collapse = ''),
  r0 = 2,
  n_gen = 14,
  n_pop = Inf,
  mutator = list(fun = "mutator_uniform_fun",
                 args = list(mu = 1/250)),
  fitness_evaluator = list(fun = "fitness_evaluator_homology_fun",
                           args = list(comparators = paste(rep('XXXXA', 100), collapse = ''),
                                       h2fs = "h2fs_univariate_linear_fun"))
)

x <- memoiseCache(fun = 'sim_pop', args = args1, cacheName = 'subsampling_dmat', 
                  seed = 1)
```

```{r, include = FALSE}
make_comp_df <- function(x, label = 1){
  last_gen <- x %>% filter(gen_num == max(gen_num))

  ltm <- proc.time()
  dmat <- stringdistmatrix(last_gen$the_seq, method = 'hamming')
  calc_time <- (proc.time() - ltm)[1]
  
  dmat_comp_tab <- data.frame(label = numeric(0),
                              size = numeric(0),
                              avg_hd = numeric(0),
                              p000 = numeric(0),
                              p010 = numeric(0),
                              p020 = numeric(0),
                              p030 = numeric(0),
                              p040 = numeric(0),
                              p050 = numeric(0),
                              p060 = numeric(0),
                              p070 = numeric(0),
                              p080 = numeric(0),
                              p090 = numeric(0),
                              p100 = numeric(0)
                              )
  
  avg_hd <- mean(dmat)
  deciles <- t(data.frame(quantile(dmat, (0:10)/10, type = 4)))
  names(deciles) <- c("p000", "p010", "p020", "p030", "p040", "p050", "p060",
                      "p070", "p080", "p090", "p100")
  row.names(deciles) <- NULL
  dmat_comp_tab <- rbind(dmat_comp_tab,
                         cbind(data.frame(label = label,
                                          size = length(dmat), 
                                          avg_hd = avg_hd,
                                          calc_time = calc_time),
                               deciles))
  
  for (i in 4:8){
    print(i)
    if (10^i < length(dmat)){
      subsam <- sample(as.numeric(dmat), 10^i)
      avg_hd <- mean(subsam)
      deciles <- t(data.frame(quantile(subsam, (0:10)/10, type = 4)))
      names(deciles) <- c("p000", "p010", "p020", "p030", "p040", "p050", "p060",
                          "p070", "p080", "p090", "p100")
      row.names(deciles) <- NULL
      dmat_comp_tab <- rbind(dmat_comp_tab,
                             cbind(data.frame(label = label,
                                              size = i, 
                                              avg_hd = avg_hd,
                                              calc_time = 0),
                                   deciles))
    }
  }
  i <- 1000
  for (i in c(100, 500, 1000, 2000, 5000, 10000)){
    if (i < length(last_gen$the_seq)){
      c_the_seq <- sample(last_gen$the_seq, i, replace = FALSE)
      ltm <- proc.time()
      c_dmat <- stringdistmatrix(c_the_seq, method = 'hamming')
      calc_time <- (proc.time() - ltm)[1]
      subsam <- as.numeric(c_dmat)
      avg_hd <- mean(subsam)
      deciles <- t(data.frame(quantile(subsam, (0:10)/10, type = 4)))
      names(deciles) <- c("p000", "p010", "p020", "p030", "p040", "p050", "p060",
                          "p070", "p080", "p090", "p100")
      row.names(deciles) <- NULL
      dmat_comp_tab <- rbind(dmat_comp_tab,
                             cbind(data.frame(label = label,
                                              size = i, 
                                              avg_hd = avg_hd,
                                              calc_time = calc_time),
                                   deciles))
    }
  }
  return(dmat_comp_tab)
}
```

```{r}
dmat_comp_tab <- NULL
for (i in 1:10){
  print(paste("==== ", i, " ====", sep = ''))
  x <- memoiseCache(fun = 'sim_pop', args = args1, cacheName = 'subsampling_dmat', 
                    seed = i)
  dmat_comp_tab <- rbind(dmat_comp_tab, make_comp_df(x, i))
}

print(dmat_comp_tab)

#write.csv(dmat_comp_tab, "/home/phillipl/projects/quasispecies_sim_reports/builds/subsampling_dmat/dmat_comp_tab_gen13_n10.csv", row.names = FALSE)
#write.csv(dmat_comp_tab, "/home/phillipl/projects/quasispecies_sim_reports/builds/subsampling_dmat/dmat_comp_tab_gen14_n10.csv", row.names = FALSE)
```

What about subsampling on the sequences to speed up the computation of the distance matrix?

Note that in practice we will always subsample down to at most 5000 since that is the limitation of the sequencing.

Some plots comparing the distance summaries

```{r}
normalized_tab <- dmat_comp_tab[0,]
i <- unique(dmat_comp_tab$label)[1]
for (i in unique(dmat_comp_tab$label)){
  c_rows <- dmat_comp_tab %>% filter(label == i)
  new_rows <- c_rows[2:nrow(c_rows),]
  new_rows[,c(-1,-2)] <- c_rows[2:nrow(c_rows),c(-1,-2)]/c_rows[rep(1, nrow(c_rows)-1),c(-1,-2)]
  normalized_tab <- rbind(normalized_tab,
                          new_rows)
}

metrics_tab <-
normalized_tab %>% 
  group_by(size) %>%
  summarize(avg = mean(avg_hd),
            sd = sd(avg_hd),
            calc_time = mean(calc_time),
            sd_10 = sd(`10%`),
            sd_50 = sd(`50%`),
            sd_90 = sd(`90%`))
```
